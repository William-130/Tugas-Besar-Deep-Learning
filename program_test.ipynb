{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febce80d",
   "metadata": {},
   "source": [
    "# üß™ FaceNet Model Evaluation & Inference\n",
    "\n",
    "Notebook ini berisi program inferensi untuk mengevaluasi performa model FaceNet.\n",
    "\n",
    "**Metrik yang dihitung:**\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- Confusion Matrix\n",
    "\n",
    "**Model:** FaceNet (InceptionResnetV1) dengan transfer learning dari VGGFace2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40888cc8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d192cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58161c9d",
   "metadata": {},
   "source": [
    "## 2. Define Model Architecture\n",
    "\n",
    "Arsitektur classifier yang sama dengan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    \"\"\"Dataset untuk embeddings\"\"\"\n",
    "    def __init__(self, embeddings, labels, label_to_idx):\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.LongTensor([label_to_idx[label] for label in labels])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"Classifier head untuk embeddings\"\"\"\n",
    "    def __init__(self, embedding_dim=512, num_classes=None, dropout_rate=0.5):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e33353",
   "metadata": {},
   "source": [
    "## 3. Load Saved Model\n",
    "\n",
    "Memuat model FaceNet yang sudah dilatih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PKL_PATH = './models/facenet_model_20251201_225633.pkl'\n",
    "MODEL_PTH_PATH = './models/facenet_classifier_20251201_225633.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {DEVICE}\")\n",
    "print(f\"üìÅ Model PKL: {MODEL_PKL_PATH}\")\n",
    "print(f\"üìÅ Model PTH: {MODEL_PTH_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48af50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model data (embeddings, labels, mappings)\n",
    "print(\"üì¶ Loading model data...\")\n",
    "\n",
    "with open(MODEL_PKL_PATH, 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "# Extract data\n",
    "embeddings = model_data['embeddings']\n",
    "labels = model_data['labels']\n",
    "label_to_idx = model_data['label_to_idx']\n",
    "idx_to_label = model_data['idx_to_label']\n",
    "\n",
    "num_classes = len(label_to_idx)\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "print(f\"\\n‚úÖ Model data loaded!\")\n",
    "print(f\"   Total embeddings: {len(embeddings)}\")\n",
    "print(f\"   Embedding dimension: {embedding_dim}\")\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   Model type: {model_data.get('model_type', 'FaceNet')}\")\n",
    "\n",
    "if 'best_val_acc' in model_data:\n",
    "    print(f\"   Training Val Accuracy: {model_data['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classifier weights\n",
    "print(\"üì¶ Loading classifier weights...\")\n",
    "\n",
    "classifier = EmbeddingClassifier(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.5\n",
    ").to(DEVICE)\n",
    "\n",
    "classifier.load_state_dict(torch.load(MODEL_PTH_PATH, map_location=DEVICE))\n",
    "classifier.eval()\n",
    "\n",
    "print(f\"‚úÖ Classifier loaded successfully!\")\n",
    "print(f\"   Architecture: 512 ‚Üí 256 ‚Üí 128 ‚Üí {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a22fe6",
   "metadata": {},
   "source": [
    "## 4. Prepare Validation Dataset\n",
    "\n",
    "Menggunakan split yang sama dengan training (20% validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ab1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data dengan random_state yang sama dengan training\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    embeddings, labels,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training set: {len(X_train)} samples\")\n",
    "print(f\"   Validation set: {len(X_val)} samples\")\n",
    "print(f\"   Split ratio: {VALIDATION_SPLIT*100:.0f}% validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation DataLoader\n",
    "val_dataset = EmbeddingDataset(X_val, y_val, label_to_idx)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Validation DataLoader created\")\n",
    "print(f\"   Batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7159d2",
   "metadata": {},
   "source": [
    "## 5. Run Inference on Validation Set\n",
    "\n",
    "Menjalankan inferensi untuk mendapatkan prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "print(\"üîç Running inference on validation set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for embeddings_batch, labels_batch in tqdm(val_loader, desc=\"Inference\"):\n",
    "        embeddings_batch = embeddings_batch.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(embeddings_batch)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"\\n‚úÖ Inference completed!\")\n",
    "print(f\"   Total predictions: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91295712",
   "metadata": {},
   "source": [
    "## 6. Calculate Evaluation Metrics\n",
    "\n",
    "Menghitung Accuracy, Precision, Recall, dan F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15262c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "print(\"üìä Calculating Evaluation Metrics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall Metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision_macro = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "precision_weighted = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nüéØ OVERALL METRICS (Validation Set)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìà Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"\\n--- Macro Average (unweighted mean across all classes) ---\")\n",
    "print(f\"   Precision: {precision_macro * 100:.2f}%\")\n",
    "print(f\"   Recall:    {recall_macro * 100:.2f}%\")\n",
    "print(f\"   F1-Score:  {f1_macro * 100:.2f}%\")\n",
    "print(f\"\\n--- Weighted Average (weighted by support/samples per class) ---\")\n",
    "print(f\"   Precision: {precision_weighted * 100:.2f}%\")\n",
    "print(f\"   Recall:    {recall_weighted * 100:.2f}%\")\n",
    "print(f\"   F1-Score:  {f1_weighted * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)',\n",
    "               'Precision (Weighted)', 'Recall (Weighted)', 'F1-Score (Weighted)'],\n",
    "    'Score': [accuracy, precision_macro, recall_macro, f1_macro,\n",
    "              precision_weighted, recall_weighted, f1_weighted],\n",
    "    'Percentage': [f\"{accuracy*100:.2f}%\", f\"{precision_macro*100:.2f}%\", \n",
    "                   f\"{recall_macro*100:.2f}%\", f\"{f1_macro*100:.2f}%\",\n",
    "                   f\"{precision_weighted*100:.2f}%\", f\"{recall_weighted*100:.2f}%\", \n",
    "                   f\"{f1_weighted*100:.2f}%\"]\n",
    "})\n",
    "\n",
    "print(\"\\nüìã METRICS SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bb070",
   "metadata": {},
   "source": [
    "## 7. Per-Class Metrics\n",
    "\n",
    "Menampilkan metrik untuk setiap kelas/person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f55c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = [idx_to_label[i] for i in range(num_classes)]\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision_per_class = precision_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "\n",
    "# Create per-class dataframe\n",
    "per_class_metrics = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "per_class_metrics_sorted = per_class_metrics.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä PER-CLASS METRICS (Sorted by F1-Score)\")\n",
    "print(\"=\" * 70)\n",
    "print(per_class_metrics_sorted.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84eb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report (sklearn)\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(all_labels, all_predictions, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c89b9",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25afd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for overall metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Overall Metrics\n",
    "metrics_names = ['Accuracy', 'Precision\\n(Macro)', 'Recall\\n(Macro)', 'F1-Score\\n(Macro)']\n",
    "metrics_values = [accuracy, precision_macro, recall_macro, f1_macro]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "bars = axes[0].bar(metrics_names, metrics_values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('üìä FaceNet Model - Overall Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{val*100:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Macro vs Weighted\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "macro_values = [precision_macro, recall_macro, f1_macro]\n",
    "weighted_values = [precision_weighted, recall_weighted, f1_weighted]\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, macro_values, width, label='Macro', color='#3498db', edgecolor='black')\n",
    "bars2 = axes[1].bar(x + width/2, weighted_values, width, label='Weighted', color='#e67e22', edgecolor='black')\n",
    "\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('üìà Macro vs Weighted Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(['Precision', 'Recall', 'F1-Score'])\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "axes[1].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, macro_values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{val*100:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "for bar, val in zip(bars2, weighted_values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{val*100:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Chart saved as 'evaluation_metrics.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df686928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (Top 20 classes for readability)\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Get top 20 classes by support\n",
    "class_support = np.bincount(all_labels, minlength=num_classes)\n",
    "top_20_indices = np.argsort(class_support)[-20:]\n",
    "\n",
    "# Filter confusion matrix\n",
    "cm_top20 = cm[np.ix_(top_20_indices, top_20_indices)]\n",
    "top_20_names = [idx_to_label[i][:15] for i in top_20_indices]  # Truncate names\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_top20, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=top_20_names, yticklabels=top_20_names)\n",
    "plt.title('üîç Confusion Matrix (Top 20 Classes)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and Bottom performers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 10 performers\n",
    "top_10 = per_class_metrics_sorted.head(10)\n",
    "axes[0].barh(top_10['Class'], top_10['F1-Score'], color='#2ecc71', edgecolor='black')\n",
    "axes[0].set_xlim(0, 1.1)\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('üèÜ Top 10 Best Performing Classes', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "for i, (idx, row) in enumerate(top_10.iterrows()):\n",
    "    axes[0].text(row['F1-Score'] + 0.02, i, f\"{row['F1-Score']*100:.1f}%\", va='center', fontsize=10)\n",
    "\n",
    "# Bottom 10 performers\n",
    "bottom_10 = per_class_metrics_sorted.tail(10)\n",
    "axes[1].barh(bottom_10['Class'], bottom_10['F1-Score'], color='#e74c3c', edgecolor='black')\n",
    "axes[1].set_xlim(0, 1.1)\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('‚ö†Ô∏è Bottom 10 Classes (Need Improvement)', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "for i, (idx, row) in enumerate(bottom_10.iterrows()):\n",
    "    axes[1].text(row['F1-Score'] + 0.02, i, f\"{row['F1-Score']*100:.1f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_bottom_performers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Top/Bottom performers chart saved as 'top_bottom_performers.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c13e7c",
   "metadata": {},
   "source": [
    "## 9. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã FACENET MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üèóÔ∏è  MODEL ARCHITECTURE:\n",
    "    - Base Model: InceptionResnetV1 (FaceNet)\n",
    "    - Pre-trained on: VGGFace2 (3.3M images, 9131 identities)\n",
    "    - Classifier: 512 ‚Üí 256 ‚Üí 128 ‚Üí {num_classes} classes\n",
    "    - Total embeddings: {len(embeddings)}\n",
    "\n",
    "üìä DATASET:\n",
    "    - Total samples: {len(embeddings)}\n",
    "    - Training samples: {len(X_train)}\n",
    "    - Validation samples: {len(X_val)}\n",
    "    - Number of classes: {num_classes}\n",
    "\n",
    "üéØ EVALUATION RESULTS (Validation Set):\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Metric              ‚îÇ  Score          ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "    ‚îÇ  Accuracy            ‚îÇ  {accuracy*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  Precision (Macro)   ‚îÇ  {precision_macro*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  Recall (Macro)      ‚îÇ  {recall_macro*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  F1-Score (Macro)    ‚îÇ  {f1_macro*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  Precision (Weighted)‚îÇ  {precision_weighted*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  Recall (Weighted)   ‚îÇ  {recall_weighted*100:6.2f}%        ‚îÇ\n",
    "    ‚îÇ  F1-Score (Weighted) ‚îÇ  {f1_weighted*100:6.2f}%        ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "üìà INTERPRETATION:\n",
    "    - Accuracy: Overall correct predictions\n",
    "    - Precision: Out of all positive predictions, how many were correct\n",
    "    - Recall: Out of all actual positives, how many were identified\n",
    "    - F1-Score: Harmonic mean of Precision and Recall\n",
    "\n",
    "‚úÖ Model uses Transfer Learning from VGGFace2 pretrained weights\n",
    "‚úÖ This is NOT zero-shot learning - requires training data per class\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Evaluation completed successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "metrics_summary.to_csv('evaluation_metrics_summary.csv', index=False)\n",
    "per_class_metrics.to_csv('per_class_metrics.csv', index=False)\n",
    "\n",
    "print(\"üíæ Metrics saved to:\")\n",
    "print(\"   - evaluation_metrics_summary.csv\")\n",
    "print(\"   - per_class_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
